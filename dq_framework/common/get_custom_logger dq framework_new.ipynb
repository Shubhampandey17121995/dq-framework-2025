{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7fdfc7-4055-45a5-a54b-a96cf6b98552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Custom Logger\n",
    "import logging\n",
    "\n",
    "def get_custom_logger(\n",
    "    v_Notebook_Name, v_Pipeline_RunId=\"None\", v_Actvt_RunId=\"None\", v_Log_Level=\"INFO\"\n",
    "):\n",
    "    if v_Log_Level.upper() == \"INFO\":\n",
    "        v_Log_Level = logging.INFO\n",
    "    elif v_Log_Level.upper() == \"WARNING\":\n",
    "        v_Log_Level = logging.WARNING\n",
    "    elif v_Log_Level.upper() == \"DEBUG\":\n",
    "        v_Log_Level = logging.DEBUG\n",
    "    elif v_Log_Level.upper() == \"ERROR\":\n",
    "        v_Log_Level = logging.ERROR\n",
    "    elif v_Log_Level.upper() == \"CRITICAL\":\n",
    "        v_Log_Level = logging.CRITICAL\n",
    "    else:\n",
    "        v_Log_Level = logging.INFO\n",
    "\n",
    "    FORMAT = (\n",
    "        \"%(asctime)s - [pipeline.runid=\"\n",
    "        + v_Pipeline_RunId\n",
    "        + \", pipeline.activity.runid=\"\n",
    "        + v_Actvt_RunId\n",
    "        + \"] - %(name)s:%(lineno)d - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    formatter = logging.Formatter(fmt=FORMAT)\n",
    "\n",
    "    logger = logging.getLogger(v_Notebook_Name)\n",
    "    logger.setLevel(v_Log_Level)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "    v_Logger_Response = (\n",
    "        \"Created a logger object with logging level=\"\n",
    "        + str(v_Log_Level)\n",
    "        + \" [10=DEBUG, 20=INFO, 30=WARNING, 40=ERROR, 50=CRITICAL]\"\n",
    "    )\n",
    "    logger.info(v_Logger_Response)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b981edbf-3d27-4643-aab0-074cae8bc5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 12:20:16,106 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:42 - INFO - Created a logger object with logging level=10 [10=DEBUG, 20=INFO, 30=WARNING, 40=ERROR, 50=CRITICAL]\n",
      "2024-12-23 12:20:16,109 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:14 - INFO - Starting the Data Quality Framework.\n",
      "2024-12-23 12:20:16,110 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:17 - INFO - Creating Spark session.\n",
      "2024-12-23 12:20:16,133 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:23 - INFO - Spark session created successfully.\n",
      "2024-12-23 12:20:16,134 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:26 - INFO - Creating the input DataFrame.\n",
      "2024-12-23 12:20:16,224 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:32 - DEBUG - Input DataFrame:\n",
      "2024-12-23 12:20:17,116 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:38 - DEBUG - Traceback (most recent call last):\n",
      "  File \"<command-1619780761519303>\", line 36, in <module>\n",
      "    value = columns[5]\n",
      "IndexError: list index out of range\n",
      "\n",
      "2024-12-23 12:20:17,117 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:45 - INFO - Performing Null Value Check.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n",
      "|Name| Age|Country|\n",
      "+----+----+-------+\n",
      "|John|  28|    USA|\n",
      "|Jane|  35|     UK|\n",
      "| Sam|null|    USA|\n",
      "|null|  22|    USA|\n",
      "|John|  28|    USA|\n",
      "+----+----+-------+\n",
      "\n",
      "+----+---+-------+\n",
      "|Name|Age|Country|\n",
      "+----+---+-------+\n",
      "|   1|  1|      0|\n",
      "+----+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 12:20:18,481 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:55 - INFO - Performing Duplicate Record Check.\n",
      "2024-12-23 12:20:19,866 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:57 - INFO - Number of duplicate records: 1\n",
      "2024-12-23 12:20:19,867 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:65 - INFO - Performing Schema Validation.\n",
      "2024-12-23 12:20:19,867 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:68 - INFO - Schema Validation Passed.\n",
      "2024-12-23 12:20:19,868 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:79 - INFO - Performing Age Data Type Validation.\n",
      "2024-12-23 12:20:19,869 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:84 - ERROR - Age Data Type Validation Failed.\n",
      "2024-12-23 12:20:19,872 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:91 - INFO - Final Data Quality Results:\n",
      "2024-12-23 12:20:19,876 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:93 - INFO - Null Value Check: [Row(Name=1, Age=1, Country=0)]\n",
      "2024-12-23 12:20:19,893 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:93 - INFO - Duplicate Record Count: 1\n",
      "2024-12-23 12:20:19,894 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:93 - INFO - Schema Validation: Passed\n",
      "2024-12-23 12:20:19,895 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:93 - INFO - Age Data Type Validation: Failed\n",
      "2024-12-23 12:20:19,897 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:102 - INFO - Stopping the Spark session.\n",
      "2024-12-23 12:20:19,901 - [pipeline.runid=DQ_RUN_001, pipeline.activity.runid=DQ_ACT_001] - DataQualityFramework:104 - INFO - Data Quality Framework execution completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "import traceback\n",
    "\n",
    "# Initialize Logger\n",
    "logger = get_custom_logger(\n",
    "    v_Notebook_Name=\"DataQualityFramework\",\n",
    "    v_Pipeline_RunId=\"DQ_RUN_001\",\n",
    "    v_Actvt_RunId=\"DQ_ACT_001\",\n",
    "    v_Log_Level=\"DEBUG\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    logger.info(\"Starting the Data Quality Framework.\")\n",
    "\n",
    "    # Create Spark session\n",
    "    logger.info(\"Creating Spark session.\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DQFramework\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    logger.info(\"Spark session created successfully.\")\n",
    "\n",
    "    # Sample Data\n",
    "    logger.info(\"Creating the input DataFrame.\")\n",
    "    data = [(\"John\", 28, \"USA\"), (\"Jane\", 35, \"UK\"), (\"Sam\", None, \"USA\"), (None, 22, \"USA\"), (\"John\", 28, \"USA\")]\n",
    "    columns = [\"Name\", \"Age\", \"Country\"]\n",
    "    df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "    # Show the input DataFrame\n",
    "    logger.debug(\"Input DataFrame:\")\n",
    "    df.show()\n",
    "\n",
    "    try: \n",
    "\t    value = columns[5] \n",
    "    except Exception as e: \n",
    "\t    logger.debug(traceback.format_exc())\n",
    "\n",
    "    # Data Quality Checks\n",
    "    dq_results = {}\n",
    "\n",
    "    # Check 1: Null Value Check\n",
    "    try:\n",
    "        logger.info(\"Performing Null Value Check.\")\n",
    "        null_checks = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "        null_checks.show()\n",
    "        dq_results[\"Null Value Check\"] = null_checks.collect()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Null Value Check: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "\n",
    "    # Check 2: Duplicate Record Check\n",
    "    try:\n",
    "        logger.info(\"Performing Duplicate Record Check.\")\n",
    "        duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "        logger.info(f\"Number of duplicate records: {duplicate_count}\")\n",
    "        dq_results[\"Duplicate Record Count\"] = duplicate_count\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Duplicate Record Check: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "\n",
    "    # Check 3: Schema Validation (Expected Schema)\n",
    "    try:\n",
    "        logger.info(\"Performing Schema Validation.\")\n",
    "        expected_schema = [\"Name\", \"Age\", \"Country\"]\n",
    "        if set(expected_schema) == set(df.columns):\n",
    "            logger.info(\"Schema Validation Passed.\")\n",
    "            dq_results[\"Schema Validation\"] = \"Passed\"\n",
    "        else:\n",
    "            logger.error(\"Schema Validation Failed.\")\n",
    "            dq_results[\"Schema Validation\"] = \"Failed\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Schema Validation: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "\n",
    "    # Check 4: Age Column Data Type Validation\n",
    "    try:\n",
    "        logger.info(\"Performing Age Data Type Validation.\")\n",
    "        if dict(df.dtypes)[\"Age\"] == \"int\":\n",
    "            logger.info(\"Age Data Type Validation Passed.\")\n",
    "            dq_results[\"Age Data Type Validation\"] = \"Passed\"\n",
    "        else:\n",
    "            logger.error(\"Age Data Type Validation Failed.\")\n",
    "            dq_results[\"Age Data Type Validation\"] = \"Failed\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Age Data Type Validation: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "\n",
    "    # Log Final DQ Results\n",
    "    logger.info(\"Final Data Quality Results:\")\n",
    "    for check, result in dq_results.items():\n",
    "        logger.info(f\"{check}: {result}\")\n",
    "\n",
    "except Exception as main_e:\n",
    "    logger.error(f\"An error occurred during the DQ Framework execution: {main_e}\")\n",
    "    logger.debug(traceback.format_exc())\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        # Stop Spark session\n",
    "        logger.info(\"Stopping the Spark session.\")\n",
    "        #spark.stop()\n",
    "        logger.info(\"Data Quality Framework execution completed.\")\n",
    "    except Exception as stop_e:\n",
    "        logger.error(f\"Error while stopping Spark session: {stop_e}\")\n",
    "        logger.debug(traceback.format_exc())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "get_custom_logger dq framework_new",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
